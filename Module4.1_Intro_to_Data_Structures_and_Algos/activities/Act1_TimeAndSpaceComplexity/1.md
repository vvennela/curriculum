<!--title={Time Complexity}-->

**Time complexity** allows us to describe **how the time taken to run a function grows as the size of the input of the function grows.**

In other words, in the function:

```python
value = [1, 2, 3, 4, 5]

def valueSum(value):
    sum = 0
    for i in range(value):
        sum = sum + i
    return sum
```
The amount of time this function takes to run will grow as the number of elements in the array increase. If the elements in the array `value` went up to 1,000,000, the amount of the time it would take for the function to compute would be much higher than if the array only went up to 10. 

Time complexity answers the question: "At what rate does the time increase for a function as the input increases". **However**, it does not answer the question, "How long does it take for a function to compute?",  because the answer relies on hardware, language, etc..

The rate of a function's growth can be described as **constant**, **linear**, **quadratic**, and so on (as depicted in the graph below). 

[//]: # "insert 'timecomplexity' image"

![](C:\Users\ugoch\Downloads\Big O Complexity.png)

In our function, `valueSum()`, this is how the function's growth would look like. Note that, as `n` grows, so does the execution time in a linear fashion. 

For time complexity, functions can grow in **constant time**, **linear time**, **quadratic time**, and so on. 



